{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"42z1SQqKFdtg"},"source":["# Lab 4. Probabilistic Inference\n","# Task 4.2 Diabetes Diagnosis Using NaÃ¯ve Bayes \n","## Problem Descriptions\n","Assume that we have various\n","of observations from patients such as basic personal details (age, gender, height,\n","weight etc), body test data (blood sugar, heart rate, lipoproteins etc) and symptoms\n","(fever, headache etc), we will try to predict if the patient is suffered from a certain\n","disease or the progression level of the disease.\n","We will use the diabetes dataset provided in sklearn, which contains 10 features and\n","one quantitative measure of diabetes progression.(course material)\n","\n","## Implementation and Results"]},{"cell_type":"code","metadata":{"id":"y6A3ZHOwFOPk","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1599832338787,"user_tz":-60,"elapsed":5192,"user":{"displayName":"Yongmin Li","photoUrl":"","userId":"09497325908540852941"}},"outputId":"ee744234-8f9f-4308-aa3d-ff1e2057e02e"},"source":["!pip install sklearn\n","from sklearn import datasets\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.model_selection import train_test_split, cross_validate\n","\n","# import numpy as np\n","import matplotlib.pyplot as plt\n","# from matplotlib import patches\n","import math"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.16.0)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qmR8h6iHGXQJ","colab":{"base_uri":"https://localhost:8080/","height":714},"executionInfo":{"status":"ok","timestamp":1599832570068,"user_tz":-60,"elapsed":677,"user":{"displayName":"Yongmin Li","photoUrl":"","userId":"09497325908540852941"}},"outputId":"ec453776-71e3-4b7f-b266-cf3c3831e9ef"},"source":["diabetes = datasets.load_diabetes()\n","# X = diabetes.data[:,[2,3,9]]\n","X = diabetes.data\n","Y = [math.floor(x/150) for x in diabetes.target]\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n","\n","nb = GaussianNB()\n","nb.fit(X_train, Y_train)\n","Y_pred = nb.predict(X_test)\n","acc = accuracy_score(Y_test, Y_pred)\n","cm = confusion_matrix(Y_test, Y_pred)\n","cr = classification_report(Y_test, Y_pred)\n","\n","print(\"Accuracy:\", acc)\n","print(\"Confusion Matrix:\\n\", cm)\n","print(\"Prior:\\n\", nb.class_prior_)\n","print(\"Mean:\\n\", nb.theta_)\n","print(\"Variance:\\n\", nb.sigma_)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.5765765765765766\n","Confusion Matrix:\n"," [[44 20  0]\n"," [21 20  4]\n"," [ 1  1  0]]\n","Prior:\n"," [0.52567976 0.43806647 0.03625378]\n","Mean:\n"," [[-0.00566069 -0.00026772 -0.02115285 -0.01479883 -0.00689089 -0.00573395\n","   0.01785399 -0.01777262 -0.02305142 -0.01417056]\n"," [ 0.00813878  0.00071837  0.02121507  0.0166012   0.01194385  0.00990548\n","  -0.01826415  0.01971383  0.0269613   0.01554786]\n"," [ 0.01476712  0.0189062   0.08648588  0.03765218  0.00198559 -0.00136607\n","  -0.03971921  0.04045905  0.04584685  0.04655653]]\n","Variance:\n"," [[0.00225479 0.00226076 0.00141088 0.00176366 0.00226593 0.00226513\n","  0.0025671  0.00167838 0.00150926 0.00196946]\n"," [0.00218156 0.00226627 0.00190433 0.00216413 0.00211941 0.00207609\n","  0.00119865 0.00185704 0.00184259 0.00231561]\n"," [0.00176259 0.00201916 0.00169741 0.00245455 0.00156965 0.00122023\n","  0.00052411 0.00200471 0.00119533 0.00144262]]\n","[ 0.03807591  0.05068012  0.06169621  0.02187235 -0.0442235  -0.03482076\n"," -0.04340085 -0.00259226  0.01990842 -0.01764613] 151.0\n","[-0.00188202 -0.04464164 -0.05147406 -0.02632783 -0.00844872 -0.01916334\n","  0.07441156 -0.03949338 -0.06832974 -0.09220405] 75.0\n","[ 0.08529891  0.05068012  0.04445121 -0.00567061 -0.04559945 -0.03419447\n"," -0.03235593 -0.00259226  0.00286377 -0.02593034] 141.0\n","[-0.08906294 -0.04464164 -0.01159501 -0.03665645  0.01219057  0.02499059\n"," -0.03603757  0.03430886  0.02269202 -0.00936191] 206.0\n","[ 0.00538306 -0.04464164 -0.03638469  0.02187235  0.00393485  0.01559614\n","  0.00814208 -0.00259226 -0.03199144 -0.04664087] 135.0\n","[-0.09269548 -0.04464164 -0.04069594 -0.01944209 -0.06899065 -0.07928784\n","  0.04127682 -0.0763945  -0.04118039 -0.09634616] 97.0\n","[-0.04547248  0.05068012 -0.04716281 -0.01599922 -0.04009564 -0.02480001\n","  0.00077881 -0.03949338 -0.06291295 -0.03835666] 138.0\n","[ 0.06350368  0.05068012 -0.00189471  0.06662967  0.09061988  0.10891438\n","  0.02286863  0.01770335 -0.03581673  0.00306441] 63.0\n","[ 0.04170844  0.05068012  0.06169621 -0.04009932 -0.01395254  0.00620169\n"," -0.02867429 -0.00259226 -0.01495648  0.01134862] 110.0\n","[-0.07090025 -0.04464164  0.03906215 -0.03321358 -0.01257658 -0.03450761\n"," -0.02499266 -0.00259226  0.06773633 -0.01350402] 310.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"q8XVMr4NFy8v"},"source":["## Discussions\n","\n","the accuracy of our model is 0.57.\n","we used naive bayes in above programme. \n","definition of the naive bayes from wikepedia is as follows:--\n","\n","* Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression, which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.\n","\n","\n","to our trained model we pass the inputs which is factors influencing diabities and as a ouput we het the probability of person being diabatic."]}]}